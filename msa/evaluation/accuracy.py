"""
Answer accuracy scoring for the multi-step agent evaluation framework.
"""
import logging
import re
from typing import List, Dict, Any, Tuple
from difflib import SequenceMatcher

log = logging.getLogger(__name__)


def evaluate_answer_accuracy(predicted_answer: str, ground_truth: str) -> Dict[str, Any]:
    """Evaluate answer accuracy against ground truth.
    
    Args:
        predicted_answer: The answer generated by the agent
        ground_truth: The reference correct answer
        
    Returns:
        Dict containing accuracy metrics including:
        - exact_match: Boolean indicating exact string match
        - similarity_score: Float between 0-1 indicating similarity
        - key_facts_match: Float between 0-1 indicating key facts coverage
        - overall_score: Weighted combination of metrics
        
    Notes:
        1. Normalize both predicted_answer and ground_truth by stripping whitespace and converting to lowercase
        2. Calculate exact_match by comparing the normalized strings
        3. Compute similarity_score using SequenceMatcher on the normalized strings
        4. Extract key facts from both predicted_answer and ground_truth using _extract_key_facts
        5. Calculate key_facts_match using _calculate_facts_coverage to measure how many ground truth facts are matched in predicted facts
        6. Compute overall_score as a weighted combination: 30% exact_match, 40% similarity_score, 30% key_facts_match
    """
    _msg = "evaluate_answer_accuracy starting"
    log.debug(_msg)
    
    # Calculate exact match
    exact_match = predicted_answer.strip().lower() == ground_truth.strip().lower()
    
    # Calculate similarity score using SequenceMatcher
    similarity_score = SequenceMatcher(
        None, 
        predicted_answer.strip().lower(), 
        ground_truth.strip().lower()
    ).ratio()
    
    # Extract key facts and compare
    predicted_facts = _extract_key_facts(predicted_answer)
    ground_truth_facts = _extract_key_facts(ground_truth)
    key_facts_match = _calculate_facts_coverage(predicted_facts, ground_truth_facts)
    
    # Calculate overall score (weighted average)
    overall_score = (
        0.3 * float(exact_match) + 
        0.4 * similarity_score + 
        0.3 * key_facts_match
    )
    
    result = {
        "exact_match": exact_match,
        "similarity_score": similarity_score,
        "key_facts_match": key_facts_match,
        "overall_score": overall_score
    }
    
    _msg = "evaluate_answer_accuracy returning"
    log.debug(_msg)
    return result


def _extract_key_facts(text: str) -> List[str]:
    """Extract key facts from text.
    
    Args:
        text: Input text to extract facts from
        
    Returns:
        List of extracted key facts as strings
        
    Notes:
        1. Split text into sentences using punctuation (.!?)
        2. Filter out sentences with length <= 10 characters
        3. Exclude sentences containing common filler phrases like "I think", "I believe", "maybe", "possibly", "perhaps"
        4. Return the remaining sentences as factual content
    """
    _msg = "_extract_key_facts starting"
    log.debug(_msg)
    
    # Simple fact extraction - sentences that contain factual information
    sentences = re.split(r'[.!?]+', text)
    facts = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and len(sentence) > 10:  # Filter out very short sentences
            # Remove common filler words/phrases for fact extraction
            if not any(phrase in sentence.lower() for phrase in [
                "i think", "i believe", "maybe", "possibly", "perhaps"
            ]):
                facts.append(sentence)
    
    _msg = "_extract_key_facts returning"
    log.debug(_msg)
    return facts


def _calculate_facts_coverage(predicted_facts: List[str], ground_truth_facts: List[str]) -> float:
    """Calculate coverage of ground truth facts in predicted facts.
    
    Args:
        predicted_facts: List of facts from predicted answer
        ground_truth_facts: List of facts from ground truth answer
        
    Returns:
        Float between 0-1 indicating coverage ratio
        
    Notes:
        1. If ground_truth_facts is empty, return 1.0 if predicted_facts is also empty, otherwise 0.0
        2. Initialize matched_facts counter to 0
        3. For each ground truth fact, compare it against all predicted facts using SequenceMatcher
        4. If similarity ratio > 0.9 (90% threshold), increment matched_facts and break (avoid double counting)
        5. Return coverage as ratio of matched_facts to total ground_truth_facts
    """
    _msg = "_calculate_facts_coverage starting"
    log.debug(_msg)
    
    if not ground_truth_facts:
        return 1.0 if not predicted_facts else 0.0
    
    matched_facts = 0
    for gt_fact in ground_truth_facts:
        for pred_fact in predicted_facts:
            similarity = SequenceMatcher(None, gt_fact.lower(), pred_fact.lower()).ratio()
            # For facts to be considered matching, they need high similarity (0.9 threshold)
            # to avoid false positives like "Java" vs "Python" being considered similar
            if similarity > 0.9:  # 90% similarity threshold for fact matching
                matched_facts += 1
                break
    
    coverage = matched_facts / len(ground_truth_facts)
    
    _msg = "_calculate_facts_coverage returning"
    log.debug(_msg)
    return coverage
